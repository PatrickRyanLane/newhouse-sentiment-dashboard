name: Daily CEO News Sentiment

on:
  schedule:
    - cron: "0 12 * * *"   # ~8am ET (adjust as needed)
  workflow_dispatch:
  push:
    paths:
      - 'news_sentiment_ceos.py'
      - 'news_articles_ceos.py'
      - 'data/**'
      - '.github/workflows/daily_ceos.yml'

permissions:
  contents: write

# Prevent overlapping runs
concurrency:
  group: daily-ceos-${{ github.ref }}
  cancel-in-progress: true

jobs:
  run:
    runs-on: ubuntu-latest
    env:
      PYTHONUNBUFFERED: '1'
      TZ: America/New_York

    steps:
      - name: Check out repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: pip
          cache-dependency-path: requirements.txt

      # If you keep a requirements.txt, we use it
      - name: Install dependencies (requirements.txt)
        if: ${{ hashFiles('requirements.txt') != '' }}
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # Otherwise install a minimal, safe set for both scripts
      - name: Install minimal deps
        if: ${{ hashFiles('requirements.txt') == '' }}
        run: |
          python -m pip install --upgrade pip
          python -m pip install pandas requests feedparser vaderSentiment

      - name: Ensure data folders exist
        run: |
          mkdir -p data
          mkdir -p data_ceos
          mkdir -p data_ceos/articles
          mkdir -p data_ceos/serp_rows

      # Make sure aliases are accessible at data/ceo_aliases.csv
      # (works no matter whether yours lives in data/, data/serps/, or repo root)
      - name: Ensure ceo_aliases at data/ceo_aliases.csv
        run: |
          if [ -f "data/ceo_aliases.csv" ]; then
            echo "Aliases already at data/ceo_aliases.csv"
          elif [ -f "data/serps/ceo_aliases.csv" ]; then
            cp -f "data/serps/ceo_aliases.csv" "data/ceo_aliases.csv"
            echo "Copied data/serps/ceo_aliases.csv -> data/ceo_aliases.csv"
          elif [ -f "ceo_aliases.csv" ]; then
            cp -f "ceo_aliases.csv" "data/ceo_aliases.csv"
            echo "Copied ceo_aliases.csv -> data/ceo_aliases.csv"
          else
            echo "WARNING: ceo_aliases.csv not found in data/, data/serps/, or repo root."
          fi

      # Normalize the daily counts (this overwrites data_ceos/daily_counts.csv)
      - name: Run daily CEO sentiment (counts)
  run: |
    python news_sentiment_ceos.py \
      --aliases data/ceo_aliases.csv \
      --articles-dir data_ceos/articles \
      --daily-dir data_ceos \
      --out data_ceos/daily_counts.csv


      # Build today's articles (Google News RSS) â†’ data_ceos/articles/YYYY-MM-DD-articles.csv
      - name: Build daily CEO articles (Google News RSS)
        env:
          ARTICLES_MAX_PER_ALIAS: "25"   # adjust if you want fewer/more per CEO
          ARTICLES_SLEEP_SEC: "0.35"     # polite delay between RSS fetches
          # ARTICLES_DATE: "YYYY-MM-DD"  # optional override for backfills
        run: |
          python news_articles_ceos.py

      # Commit only if there are changes
      - name: Commit & push changes (if any)
        run: |
          git config user.name  "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git config --global --add safe.directory "$GITHUB_WORKSPACE"

          git add -A
          if git diff --cached --quiet; then
            echo "No changes to commit."
          else
            git commit -m "chore(data_ceos): update CEO counts & articles for $(date -u +%F)"
            # Rebase in case other bots/users pushed meanwhile
            git pull --rebase --autostash || true
            git push
          fi
